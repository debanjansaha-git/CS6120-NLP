{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from byte_pair_encoding.bpe import BytePairEncoder\n",
    "import nltk\n",
    "import os\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained BPE vocabulary\n",
    "vocab_file = 'dataset/train/bpe_train_vocab.txt'\n",
    "bpe = BytePairEncoder()\n",
    "with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "    vocab = {}\n",
    "    for line in f:\n",
    "        token, freq = line.strip().split(': ')\n",
    "        vocab[token] = int(freq)\n",
    "    bpe.vocab = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the test books\n",
    "test_dir = 'dataset/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLTK's Punkt tokenizer\n",
    "def_tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
    "# Store a reference to the test default tokens\n",
    "ref_test_tokens = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: sherlock-holmes.txt\n",
      "Original Text:\n",
      "﻿The Project Gutenberg eBook of The Adventures of Sherlock Holmes\n",
      "    \n",
      "This ebook is for the use of ...\n",
      "Decoded Text:\n",
      "﻿T<31> Project Gu<4416><106>rg eBook <13> T<31> Adventures <13> <15600><31>rlock Holmes\n",
      "    \n",
      "This e<...\n",
      "\n",
      "File: frankenstein.txt\n",
      "Original Text:\n",
      "﻿The Project Gutenberg eBook of Frankenstein; Or, The Modern Prometheus\n",
      "    \n",
      "This ebook is for the u...\n",
      "Decoded Text:\n",
      "﻿T<31> Project Gu<4416><106>rg eBook <13> <15345>rank<10387>ste<121>; Or<9431> T<31> <15792>o<15751>...\n",
      "\n",
      "File: dracula.txt\n",
      "Original Text:\n",
      "﻿The Project Gutenberg eBook of Dracula\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the Uni...\n",
      "Decoded Text:\n",
      "﻿T<31> Project Gu<4416><106>rg eBook <13> <12748>acula\n",
      "    \n",
      "This e<1807> is for <18> <901>e <13> any...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test books\n",
    "total_tokens = 0\n",
    "correct_tokens = 0\n",
    "total_words = 0\n",
    "\n",
    "for file_name in os.listdir(test_dir):\n",
    "    with open(os.path.join(test_dir, file_name), 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "        # Encode the text using the trained BPE vocabulary\n",
    "        encoded_text = bpe.encode(text)\n",
    "\n",
    "        # Decode the encoded text\n",
    "        decoded_text = bpe.decode(encoded_text)\n",
    "\n",
    "        # BPE Tokenization accuracy and coverage\n",
    "        original_tokens = len(text.split())\n",
    "        total_tokens += original_tokens\n",
    "        decoded_tokens = len(decoded_text.split())\n",
    "        correct_tokens += sum(1 for token in decoded_text.split() if token.isdigit())\n",
    "        total_words += len(decoded_text.split())\n",
    "\n",
    "        print(f\"File: {file_name}\")\n",
    "        print(f\"Original Text:\\n{text[:100]}...\")\n",
    "        print(f\"Decoded Text:\\n{decoded_text[:100]}...\")\n",
    "        print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPE Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.40\n",
      "Recall: 0.40\n",
      "F1 Score: 0.40\n",
      "Jaccard Similarity: 0.25\n",
      "Tokenization Accuracy: 0.04%\n",
      "Coverage: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "original_tokens_set = set(text.split())\n",
    "decoded_tokens_set = set(decoded_text.split())\n",
    "\n",
    "TP = len(decoded_tokens_set.intersection(original_tokens_set))\n",
    "FP = len(decoded_tokens_set - original_tokens_set)\n",
    "FN = len(original_tokens_set - decoded_tokens_set)\n",
    "\n",
    "# Calculate precision, recall, F1 score\n",
    "precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "# Calculate Jaccard similarity\n",
    "jaccard_similarity = len(decoded_tokens_set.intersection(original_tokens_set)) / len(decoded_tokens_set.union(original_tokens_set))\n",
    "# Calculate tokenization accuracy\n",
    "tokenization_accuracy = correct_tokens / total_tokens * 100\n",
    "# Calculate tokenization coverage\n",
    "coverage = (len(decoded_tokens_set) / len(original_tokens_set)) * 100\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1_score:.2f}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_similarity:.2f}\")\n",
    "print(f\"Tokenization Accuracy: {tokenization_accuracy:.2f}%\")\n",
    "print(f\"Coverage: {coverage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(test_dir):\n",
    "    with open(os.path.join(test_dir, file_name), 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "        # Tokenize with the default punkt tokenizer\n",
    "        def_tokenized_text = def_tokenizer.tokenize(text)\n",
    "        ref_test_tokens[file_name] = def_tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Reference Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference tokenization saved to dataset/reference_punkt_tokens.json\n"
     ]
    }
   ],
   "source": [
    "# Save the tokenized results in a structured format\n",
    "output_file = 'dataset/reference_punkt_tokens.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(ref_test_tokens, file, indent=4)\n",
    "\n",
    "print(f\"Reference tokenization saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference tokenization\n",
    "with open('dataset/reference_punkt_tokens.json', 'r', encoding='utf-8') as file:\n",
    "    reference_tokenization = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for evaluation metrics\n",
    "total_tokens_bpe = 0\n",
    "correct_tokens_bpe = 0\n",
    "total_tokens_default = 0\n",
    "correct_tokens_default = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BPE algorithm and NLTK's default tokenizer\n",
    "for file_name, tokenized_text in reference_tokenization.items():\n",
    "    with open(os.path.join(test_dir, file_name), 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "        # Tokenize with BPE algorithm\n",
    "        encoded_text = bpe.encode(text)\n",
    "        decoded_text = bpe.decode(encoded_text)\n",
    "        decoded_tokens_bpe = decoded_text.split()\n",
    "\n",
    "        # Tokenize with NLTK's default method\n",
    "        default_tokens = tokenizer(text)\n",
    "\n",
    "        # Calculate metrics for BPE\n",
    "        total_tokens_bpe += len(decoded_tokens_bpe)\n",
    "        correct_tokens_bpe += sum(1 for token in decoded_tokens_bpe if token in tokenized_text)\n",
    "\n",
    "        # Calculate metrics for NLTK's default method\n",
    "        total_tokens_default += len(default_tokens)\n",
    "        correct_tokens_default += sum(1 for token in default_tokens if token in tokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy, coverage, precision, recall, F1 score, and Jaccard similarity for both methods\n",
    "accuracy_bpe = (correct_tokens_bpe / total_tokens_bpe) * 100 if total_tokens_bpe > 0 else 0\n",
    "accuracy_default = (correct_tokens_default / total_tokens_default) * 100 if total_tokens_default > 0 else 0\n",
    "\n",
    "precision_bpe = (correct_tokens_bpe / total_tokens_bpe) if total_tokens_bpe > 0 else 0\n",
    "precision_default = (correct_tokens_default / total_tokens_default) if total_tokens_default > 0 else 0\n",
    "\n",
    "recall_bpe = (correct_tokens_bpe / sum(len(tokenized_text) for tokenized_text in reference_tokenization.values())) if total_tokens_bpe > 0 else 0\n",
    "recall_default = (correct_tokens_default / sum(len(tokenized_text) for tokenized_text in reference_tokenization.values())) if total_tokens_default > 0 else 0\n",
    "\n",
    "f1_score_bpe = 2 * (precision_bpe * recall_bpe) / (precision_bpe + recall_bpe) if (precision_bpe + recall_bpe) > 0 else 0\n",
    "f1_score_default = 2 * (precision_default * recall_default) / (precision_default + recall_default) if (precision_default + recall_default) > 0 else 0\n",
    "\n",
    "jaccard_similarity_bpe = correct_tokens_bpe / total_tokens_bpe if total_tokens_bpe > 0 else 0\n",
    "jaccard_similarity_default = correct_tokens_default / total_tokens_default if total_tokens_default > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Algorithm Metrics:\n",
      "Accuracy: 0.24%\n",
      "Precision: 0.00\n",
      "Recall: 0.05\n",
      "F1 Score: 0.00\n",
      "Jaccard Similarity: 0.00\n",
      "\n",
      "NLTK's Default Tokenizer Metrics:\n",
      "Accuracy: 0.20%\n",
      "Precision: 0.00\n",
      "Recall: 0.05\n",
      "F1 Score: 0.00\n",
      "Jaccard Similarity: 0.00\n"
     ]
    }
   ],
   "source": [
    "print(\"BPE Algorithm Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_bpe:.2f}%\")\n",
    "print(f\"Precision: {precision_bpe:.2f}\")\n",
    "print(f\"Recall: {recall_bpe:.2f}\")\n",
    "print(f\"F1 Score: {f1_score_bpe:.2f}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_similarity_bpe:.2f}\")\n",
    "\n",
    "print(\"\\nNLTK's Default Tokenizer Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_default:.2f}%\")\n",
    "print(f\"Precision: {precision_default:.2f}\")\n",
    "print(f\"Recall: {recall_default:.2f}\")\n",
    "print(f\"F1 Score: {f1_score_default:.2f}\")\n",
    "print(f\"Jaccard Similarity: {jaccard_similarity_default:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
